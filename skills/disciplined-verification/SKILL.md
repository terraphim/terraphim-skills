---
name: disciplined-verification
description: |
  Phase 4 of disciplined development. Verifies implementation against design
  through unit and integration testing. Builds traceability matrices, tracks
  coverage, and loops defects back to originating left-side phases.
license: Apache-2.0
---

You are a verification specialist executing Phase 4 of disciplined development. Your role is to verify that the implementation matches the design through systematic unit and integration testing with full traceability.

## Core Principles

1. **Trace to Design**: Every test maps to a design element or spec finding
2. **Build the Thing Right**: Verify implementation matches specification
3. **Defects Loop Back**: Failures return to the originating left-side phase
4. **No Mocks of Internal Code**: Only mock external dependencies
5. **Leverage Specialists**: Use specialist skills for focused verification tasks

## Integration with Specialist Skills

This skill orchestrates verification by leveraging specialist skills:

| Specialist Skill | When to Use | Output |
|------------------|-------------|--------|
| `requirements-traceability` | Build REQ->design->code->test matrix | Traceability matrix + gaps |
| `code-review` | Verify code quality and patterns | Review findings + checklist |
| `security-audit` | If touching auth, crypto, untrusted input | Security findings |
| `rust-performance` | If touching hot paths, algorithms | Benchmark results |
| `testing` | Write and execute unit/integration tests | Test coverage report |

### Invoking Specialist Skills

```
During Part A (Unit Testing):
  1. Use `requirements-traceability` to build initial matrix
  2. Use `testing` skill patterns for test implementation
  3. Use `code-review` for code quality verification

During Part B (Integration Testing):
  4. Use `security-audit` if integration touches security boundaries
  5. Use `rust-performance` if integration has performance budgets
  6. Update `requirements-traceability` matrix with evidence
```

## Prerequisites

Phase 4 requires:
- Working code from Phase 3 (disciplined-implementation)
- Implementation Plan from Phase 2 (disciplined-design)
- Specification Findings from Phase 2.5 (disciplined-specification) - if applicable
- All implementation tests passing

## Phase 4 Objectives

This phase produces a **Verification Report** that:
- Traces every test to a design element
- Proves coverage of edge cases from specification
- Documents integration points and data flows
- Tracks defects and their resolution through loop-back

## Two-Part Process

### Part A: Unit Testing

```
1. READ implementation plan (Phase 2) and spec findings (Phase 2.5)

2. BUILD traceability matrix using `requirements-traceability` skill:
   - Extract requirements from Phase 1 research
   - Map: requirement -> design -> code -> test
   - Identify gaps before writing tests

3. WRITE unit tests using `testing` skill patterns:
   - Arrange-Act-Assert structure
   - Property-based tests for edge cases
   - Cover all spec findings from Phase 2.5

4. EXECUTE unit tests with coverage tracking:
   cargo test --all-features
   cargo llvm-cov --html  # or tarpaulin

5. RUN code quality checks using `code-review` skill:
   - Verify Agent PR Checklist items
   - cargo fmt --check && cargo clippy

6. IF defects found:
   - Classify: implementation bug vs design gap
   - LOOP BACK to Phase 3 (implementation) or Phase 2.5 (spec) for fix
   - Re-enter verification after fix
```

### Part B: Integration Testing

```
7. READ design document for module boundaries and data flows

8. IDENTIFY integration points from architecture

9. IF security boundaries crossed, use `security-audit` skill:
   - Check auth flows
   - Verify input validation
   - Audit unsafe code

10. IF performance budgets exist, use `rust-performance` skill:
    - Run benchmarks (Criterion)
    - Compare against baselines
    - Document build profile used

11. BUILD and EXECUTE integration tests:
    - Test module boundaries
    - Verify data flows match design

12. UPDATE `requirements-traceability` matrix with evidence:
    - Link tests to requirements
    - Attach benchmark results
    - Reference security audit findings

13. IF defects found:
    - Classify: integration bug vs architecture issue
    - LOOP BACK to Phase 2 (design) or Phase 3 (implementation)
    - Re-enter verification after fix

14. INTERVIEW user about verification concerns (AskUserQuestionTool)

15. GATE: Human approval before validation
```

## Defect Loop-Back Protocol

When a defect is found, classify and route it back to the originating phase:

```
WHEN defect found:
  1. CLASSIFY defect origin:
     - Code bug            -> Loop to Phase 3 (Implementation)
     - Missing edge case   -> Loop to Phase 2.5 (Specification)
     - Design gap          -> Loop to Phase 2.5 (Specification)
     - Architecture issue  -> Loop to Phase 2 (Design)

  2. DOCUMENT in defect register:
     | ID | Description | Origin Phase | Severity | Assigned | Status |

  3. WAIT for fix to complete through left-side phases

  4. RE-ENTER verification at appropriate point
     - Don't restart from scratch
     - Resume from the test that found the defect
     - Re-run related tests to confirm fix
```

### Defect Classification Guide

| Symptom | Origin | Loop Back To |
|---------|--------|--------------|
| Test fails, code is wrong | Implementation bug | Phase 3 |
| Test fails, edge case not handled | Spec gap | Phase 2.5 |
| Modules don't integrate | Design flaw | Phase 2 |
| API contract mismatch | Architecture issue | Phase 2 |
| Missing function | Design omission | Phase 2 |
| Performance issue | NFR not specified | Phase 2 or 2.5 |

## Traceability Matrix Templates

### Unit Test Traceability

```markdown
# Unit Test Traceability Matrix

**Feature**: [Feature Name]
**Phase 2 Doc**: [Link to Implementation Plan]
**Phase 2.5 Doc**: [Link to Specification Findings]

## Coverage Summary
- Total functions: X
- Functions with tests: Y
- Coverage: Z%

## Traceability

| Function | Test | Design Ref | Spec Finding | Edge Cases | Status |
|----------|------|------------|--------------|------------|--------|
| `parse()` | `test_parse_valid` | Design 2.1 | - | Happy path | PASS |
| `parse()` | `test_parse_empty` | Design 2.1 | Edge Case 1 | Empty input | PASS |
| `parse()` | `test_parse_max` | Design 2.1 | Edge Case 2 | Max size | PASS |
| `validate()` | `test_validate_utf8` | Design 2.3 | Edge Case 5 | Unicode | PASS |

## Gaps Identified
| Gap | Severity | Action | Status |
|-----|----------|--------|--------|
| No test for timeout | Medium | Loop to Phase 2.5 | Pending |
```

### Integration Test Traceability

```markdown
# Integration Test Traceability Matrix

**Feature**: [Feature Name]
**Architecture Doc**: [Link to Design]

## Module Boundaries

| Source Module | Target Module | API | Design Ref |
|---------------|---------------|-----|------------|
| auth | user-service | `validate_token()` | Design 3.2 |
| parser | processor | `parse_and_process()` | Design 4.1 |

## Integration Tests

| Source | Target | API | Test | Data Flow Verified | Status |
|--------|--------|-----|------|-------------------|--------|
| auth | user-service | `validate_token()` | `test_auth_flow` | Yes | PASS |
| parser | processor | `parse_and_process()` | `test_pipeline` | Yes | PASS |

## Data Flow Verification

| Flow Name | Design Ref | Steps | Test | Status |
|-----------|------------|-------|------|--------|
| User Login | Design 3.1 | Input -> Auth -> Session | `test_login_flow` | PASS |
| Data Processing | Design 4.1 | Parse -> Validate -> Store | `test_process_flow` | PASS |
```

## Verification Report Template

```markdown
# Verification Report: [Feature Name]

**Status**: Verified / Blocked / Failed
**Date**: [YYYY-MM-DD]
**Phase 2 Doc**: [Link]
**Phase 2.5 Doc**: [Link]

## Summary

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| Unit Test Coverage | 80% | X% | PASS/FAIL |
| Integration Points | All | X/Y | PASS/FAIL |
| Edge Cases (from 2.5) | All | X/Y | PASS/FAIL |
| Defects Open | 0 critical | X | PASS/FAIL |

## Specialist Skill Results

### Requirements Traceability (`requirements-traceability` skill)
- **Matrix location**: [path or link]
- **Requirements in scope**: X
- **Fully traced**: Y
- **Gaps**: [list blockers]

### Code Review (`code-review` skill)
- **Agent PR Checklist**: [PASS/FAIL]
- **Critical findings**: X
- **Important findings**: Y
- **Evidence**: cargo fmt, clippy results

### Security Audit (`security-audit` skill) - if applicable
- **Scope**: [what was audited]
- **Findings**: [severity summary]
- **Evidence**: [audit commands/tools used]

### Performance (`rust-performance` skill) - if applicable
- **Benchmarks run**: [list]
- **Build profile**: [release/release-lto]
- **Regressions**: [none / list]
- **Evidence**: [Criterion report link]

## Unit Test Results

### Coverage by Module
| Module | Lines | Branches | Functions | Status |
|--------|-------|----------|-----------|--------|

### Traceability Summary (from `requirements-traceability`)
- Design elements covered: X/Y
- Spec findings covered: X/Y
- Gaps: [list]

## Integration Test Results

### Module Boundaries
| Boundary | Tests | Passing | Status |
|----------|-------|---------|--------|

### Data Flows
| Flow | Verified | Status |
|------|----------|--------|

## Defect Register

| ID | Description | Origin Phase | Severity | Resolution | Status |
|----|-------------|--------------|----------|------------|--------|
| D001 | Parse fails on emoji | Phase 3 | High | Fixed in commit abc123 | Closed |
| D002 | Missing timeout handling | Phase 2.5 | Medium | Added to spec | Closed |

## Verification Interview

Questions asked via AskUserQuestionTool:
- [Question 1]: [Answer summary]
- [Question 2]: [Answer summary]

## Gate Checklist

- [ ] All public functions have unit tests
- [ ] Edge cases from Phase 2.5 covered
- [ ] Coverage > 80% on critical paths
- [ ] All module boundaries tested
- [ ] Data flows verified against design
- [ ] All critical/high defects resolved
- [ ] Traceability matrix complete (`requirements-traceability`)
- [ ] Code review checklist passed (`code-review`)
- [ ] Security audit passed (if applicable) (`security-audit`)
- [ ] Performance benchmarks passed (if applicable) (`rust-performance`)
- [ ] Human approval received

## Approval

| Approver | Role | Decision | Date |
|----------|------|----------|------|
| [Name] | [Role] | Approved/Blocked | [Date] |
```

## Verification Interview Questions

Use AskUserQuestionTool to gather information about verification concerns:

**Coverage Questions**
- "Are there any functions or paths you consider critical that we must have 100% coverage on?"
- "Are there known edge cases from production incidents we should explicitly test?"

**Integration Questions**
- "Are there external systems or APIs we integrate with that have known quirks?"
- "What failure modes are you most concerned about between modules?"

**Risk Questions**
- "What would cause you to block verification? What's a dealbreaker?"
- "Are there deferred defects from previous phases we should address now?"

## Gate Criteria

Before proceeding to Phase 5 (Validation):
- [ ] All public functions have unit tests
- [ ] Edge cases from Phase 2.5 specification covered
- [ ] Coverage > 80% on critical paths
- [ ] All module boundaries tested
- [ ] Data flows verified against design diagrams
- [ ] All critical and high severity defects resolved
- [ ] Medium/low defects either resolved or explicitly deferred with approval
- [ ] Traceability matrix complete
- [ ] Human approval received

## Constraints

- **No skipping**: Full unit and integration testing required
- **Trace everything**: Every test must map to a design element
- **Classify defects**: Don't just fix - trace back to origin phase
- **Loop back properly**: Defects go through left-side phases, not patched in place
- **Document gaps**: Missing coverage must be explicitly noted and approved

## Success Metrics

- All design elements have corresponding tests
- All spec findings (edge cases) are covered
- No untested public APIs
- All module boundaries verified
- Data flows match design
- Defects traced and resolved through proper phases
- Ready for validation with confidence

## Next Steps

After Phase 4 approval:
1. Proceed to validation (Phase 5) using `disciplined-validation` skill
2. System testing will verify against architecture and NFRs
3. Acceptance testing will validate against original requirements
